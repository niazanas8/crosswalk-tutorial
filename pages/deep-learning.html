<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Deep Learning • Crosswalk Tutorial</title>
  <link rel="stylesheet" href="../assets/css/styles.css">
  <script defer src="../assets/js/nav.js"></script>
</head>
<body>
<header>
<nav class="nav wrapper">
  <a href="../index.html">Home</a>
  <a href="introduction.html">Introduction</a>
  <a href="challenges.html">Challenges</a>
  <a href="classical.html">Classical Methods</a>
  <a href="deep-learning.html" class="active">Deep Learning</a>
  <a href="multimodal.html">VLM / Multimodal</a>
  <a href="datasets.html">Datasets & Metrics</a>
  <a href="analysis.html">Comparative Analysis</a>
  <a href="future.html">Future</a>
  <a href="bibliography.html">Annotated Bibliography</a>
</nav>
</header>

<main class="wrapper">
  <h1>Deep Learning Approaches</h1>
  <p class="audio-note">Voice-over: <code>../assets/audio/deeplearning.mp3</code></p>
  <audio controls src="../assets/audio/deeplearning.mp3"></audio>
  <hr/>

  <p>
    Deep learning improved crosswalk detection by learning features directly from data instead of hand-crafted rules. Two common strategies are:
    <strong>object detection</strong> (e.g., Faster R-CNN, YOLO) to localize crosswalk regions and
    <strong>semantic segmentation</strong> (e.g., U-Net, DeepLab) to label pixels that belong to the crosswalk pattern.
    These models generally handle wear, mild occlusion, and viewpoint changes better than classical methods
    <a class="cite" href="bibliography.html#ref-2" data-cite="Haider et al., 2025 — DL pipeline">[2]</a>
    <a class="cite" href="bibliography.html#ref-7" data-cite="Yang et al., 2022 — YOLO on embedded">[7]</a>.
  </p>

  <p>
    Performance still depends on training diversity. Night scenes, heavy shadows, or new cities can cause drops due to <em>domain shift</em>. For deployment, papers report practical metrics such as
    <strong>FPS/latency</strong> and power, and explore compression/distillation for real-time use on devices like Jetson
    <a class="cite" href="bibliography.html#ref-7" data-cite="Yang et al., 2022 — accuracy vs. speed">[7]</a>.
  </p>

  <figure style="margin-top:18px">
    <img src="../assets/images/sample-image-with-main-objects-detected-using-YOLO-v11-model.webp"
         alt="YOLO-style object detection showing boxes on people and vehicles at a crosswalk"
         style="width:100%; border-radius:12px; border:1px solid #1f2a44">
    <figcaption>
      YOLO object detection example: bounding boxes on people and vehicles near a crosswalk. 
      Source: <a href="https://medium.com/@wicar/yolo-the-ai-model-powering-real-time-object-detection-b2ea7fbb4496" target="_blank" rel="noopener">Medium – YOLO: The AI Model Powering Real-Time Object Detection</a>.
    </figcaption>
  </figure>

  <div class="card" style="margin-top:18px">
    <h3 style="margin-top:0">Typical Training Notes</h3>
    <ul>
      <li>Datasets: mix urban driving sets with task-specific crosswalk crops/masks.</li>
      <li>Labels: boxes for detection; pixel masks for segmentation.</li>
      <li>Metrics: mAP (detection), mIoU/pixel accuracy (segmentation).</li>
      <li>Robustness: augment for lighting, weather, perspective; report day/night and occlusion breakdowns.</li>
    </ul>
  </div>

  <div class="card" style="margin-top:18px">
    <strong>References on this page:</strong>
    <a class="cite" href="bibliography.html#ref-2">[2]</a>
    <a class="cite" href="bibliography.html#ref-7">[7]</a>
  </div>
</main>

<footer>
  <div class="wrapper">Last updated September 28, 2025 • Built for CS661 Project 1 • Author: Anas Niaz</div>
</footer>
</body>
</html>
