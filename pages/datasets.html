<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Datasets & Metrics • Crosswalk Tutorial</title>
  <link rel="stylesheet" href="../assets/css/styles.css">
  <script defer src="../assets/js/nav.js"></script>
</head>
<body>
<header>
<nav class="nav wrapper">
  <a href="../index.html">Home</a>
  <a href="introduction.html">Introduction</a>
  <a href="challenges.html">Challenges</a>
  <a href="classical.html">Classical Methods</a>
  <a href="deep-learning.html">Deep Learning</a>
  <a href="multimodal.html">VLM / Multimodal</a>
  <a href="datasets.html" class="active">Datasets & Metrics</a>
  <a href="analysis.html">Comparative Analysis</a>
  <a href="future.html">Future</a>
  <a href="bibliography.html">Annotated Bibliography</a>
</nav>
</header>
<main class="wrapper">
  <h1>Datasets & Evaluation Metrics</h1>
  <p class="audio-note">Voice later: <code>assets/audio/datasets.mp3</code></p>
  <audio controls src="../assets/audio/datasets.mp3"></audio>
  <hr/>

  <p>
    Crosswalk research often combines general-purpose driving datasets with smaller, task-specific sets. Large public datasets help with pretraining and benchmarking (e.g., city scenes, semantic labels), while many papers also curate their own crosswalk crops or masks for fine-tuning and evaluation
    <a class="cite" href="bibliography.html#ref-1" data-cite="Romić et al., 2021 — classical baseline">[1]</a>
    <a class="cite" href="bibliography.html#ref-7" data-cite="Yang et al., 2022 — practical deployment dataset usage">[7]</a>.
  </p>

  <h2>Commonly Used Datasets</h2>
  <div class="table-wrap">
    <table class="table">
      <tr>
        <th>Dataset</th>
        <th>Type</th>
        <th>Why it’s useful</th>
      </tr>
      <tr>
        <td><strong>Cityscapes</strong></td>
        <td>Urban street scenes with pixel-level labels</td>
        <td>High-quality segmentation ground truth for roads, sidewalks, people, etc.; useful for training/validating segmentation backbones.</td>
      </tr>
      <tr>
        <td><strong>KITTI</strong></td>
        <td>Driving scenes (detection, stereo, tracking)</td>
        <td>Standard benchmarks for object detection/tracking; good for detector backbones and evaluation protocols.</td>
      </tr>
      <tr>
        <td><strong>Task-Specific Crosswalk Sets</strong></td>
        <td>Custom crops / masks from dashcams or maps</td>
        <td>Many papers build small labeled sets of crosswalk patches or masks to fine-tune detectors/segmenters on the actual target task
          <a class="cite" href="bibliography.html#ref-1" data-cite="Romić et al., 2021 — task-specific labeling">[1]</a>
          <a class="cite" href="bibliography.html#ref-7" data-cite="Yang et al., 2022 — embedded study">[7]</a>.
        </td>
      </tr>
    </table>
  </div>

  <h2 style="margin-top:22px">Evaluation Metrics</h2>
  <ul>
    <li><strong>Detection:</strong> Precision, Recall, and <em>mean Average Precision (mAP)</em> at IoU thresholds (e.g., 0.5, 0.5:0.95).</li>
    <li><strong>Segmentation:</strong> <em>Intersection-over-Union (IoU)</em> / <em>mean IoU (mIoU)</em>, and pixel accuracy for crosswalk regions.</li>
    <li><strong>Operational:</strong> Frames per second (FPS) / <em>latency</em>, memory/power—important for embedded deployment (e.g., Jetson)
      <a class="cite" href="bibliography.html#ref-7" data-cite="Yang et al., 2022 — FPS/latency reporting">[7]</a>.
