<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Datasets & Metrics • Crosswalk Tutorial</title>
  <link rel="stylesheet" href="../assets/css/styles.css">
  <script defer src="../assets/js/nav.js"></script>
</head>
<body>
<header>
<nav class="nav wrapper">
  <a href="../index.html">Home</a>
  <a href="introduction.html">Introduction</a>
  <a href="challenges.html">Challenges</a>
  <a href="classical.html">Classical Methods</a>
  <a href="deep-learning.html">Deep Learning</a>
  <a href="multimodal.html">VLM / Multimodal</a>
  <a href="datasets.html" class="active">Datasets & Metrics</a>
  <a href="analysis.html">Comparative Analysis</a>
  <a href="future.html">Future</a>
  <a href="bibliography.html">Annotated Bibliography</a>
</nav>
</header>

<main class="wrapper">
  <h1>Datasets & Evaluation Metrics</h1>
  <p class="audio-note">Voice-over: <code>../assets/audio/datasets.mp3</code></p>
  <audio controls src="../assets/audio/datasets.mp3"></audio>
  <hr/>

  <p>
    Crosswalk research often combines general-purpose driving datasets with smaller, task-specific sets. Large public datasets help with pretraining and benchmarking (e.g., city scenes, semantic labels), while many papers also curate their own crosswalk crops or masks for fine-tuning and evaluation
    <a class="cite" href="bibliography.html#ref-1" data-cite="Romić et al., 2021 — classical baseline">[1]</a>
    <a class="cite" href="bibliography.html#ref-7" data-cite="Yang et al., 2022 — practical deployment dataset usage">[7]</a>.
  </p>

  <h2>Commonly Used Datasets</h2>
  <div class="table-wrap">
    <table class="table">
      <tr>
        <th>Dataset</th>
        <th>Type</th>
        <th>Why it’s useful</th>
      </tr>
      <tr>
        <td><strong>Cityscapes</strong></td>
        <td>Urban street scenes with pixel-level labels</td>
        <td>High-quality segmentation ground truth for roads, sidewalks, people, vehicles, etc.; ideal for training/validating segmentation backbones.</td>
      </tr>
      <tr>
        <td><strong>KITTI</strong></td>
        <td>Driving scenes (detection, stereo, tracking)</td>
        <td>Standard benchmarks for detector backbones and evaluation protocols.</td>
      </tr>
      <tr>
        <td><strong>Task-Specific Crosswalk Sets</strong></td>
        <td>Custom crops / masks from dashcams or maps</td>
        <td>Many papers build small labeled sets of crosswalk patches or masks to fine-tune detectors/segmenters on the target task
          <a class="cite" href="bibliography.html#ref-1" data-cite="Romić et al., 2021 — task-specific labeling">[1]</a>
          <a class="cite" href="bibliography.html#ref-7" data-cite="Yang et al., 2022 — embedded study">[7]</a>.
        </td>
      </tr>
    </table>
  </div>

  <h2 style="margin-top:22px">Evaluation Metrics</h2>
  <ul>
    <li><strong>Detection:</strong> Precision, Recall, and <em>mean Average Precision (mAP)</em> at IoU thresholds (e.g., 0.5, 0.5:0.95).</li>
    <li><strong>Segmentation:</strong> <em>Intersection-over-Union (IoU)</em> / <em>mean IoU (mIoU)</em>, and pixel accuracy for crosswalk regions.</li>
    <li><strong>Operational:</strong> Frames per second (FPS) / <em>latency</em>, memory/power—important for embedded deployment (e.g., Jetson)
      <a class="cite" href="bibliography.html#ref-7" data-cite="Yang et al., 2022 — FPS/latency reporting">[7]</a>.
    </li>
    <li><strong>Robustness:</strong> Breakdowns by day/night, weather, occlusion level, and city/device to show generalization.</li>
  </ul>

  <!-- Figures: Cityscapes examples -->
  <figure style="margin-top:18px">
    <img src="../assets/images/OIP.jpeg"
         alt="Cityscapes dataset segmentation example 1"
         style="width:100%; border-radius:12px; border:1px solid #1f2a44">
    <figcaption>Cityscapes example with semantic labels (roads, sidewalks, people, vehicles). Source: Cityscapes dataset.</figcaption>
  </figure>

  <figure style="margin-top:18px">
    <img src="../assets/images/OIP (1).jpeg"
         alt="Cityscapes dataset segmentation example 2"
         style="width:100%; border-radius:12px; border:1px solid #1f2a44">
    <figcaption>Another annotated Cityscapes scene, commonly used to pretrain/evaluate segmentation backbones. Source: Cityscapes dataset.</figcaption>
  </figure>

  <figure style="margin-top:18px">
    <img src="../assets/images/OIP (2).jpeg"
         alt="Cityscapes dataset segmentation example 3"
         style="width:100%; border-radius:12px; border:1px solid #1f2a44">
    <figcaption>Intersection view with multiple labeled classes. Source: Cityscapes dataset.</figcaption>
  </figure>

  <figure style="margin-top:18px">
    <img src="../assets/images/zuerich00.png"
         alt="Cityscapes Zurich example with pixel-level segmentation"
         style="width:100%; border-radius:12px; border:1px solid #1f2a44">
    <figcaption>Cityscapes example from Zurich, annotated with semantic segmentation. Source: Cityscapes dataset.</figcaption>
  </figure>

  <div class="card" style="margin-top:18px">
    <strong>References on this page:</strong>
    <a class="cite" href="bibliography.html#ref-1">[1]</a>
    <a class="cite" href="bibliography.html#ref-7">[7]</a>
  </div>
</main>

<footer>
  <div class="wrapper">Last updated September 28, 2025 • Built for CS661 Project 1 • Author: Anas Niaz</div>
</footer>
</body>
</html>
