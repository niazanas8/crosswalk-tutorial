<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Datasets & Metrics • Crosswalk Tutorial</title>
  <link rel="stylesheet" href="../assets/css/styles.css">
  <script defer src="../assets/js/nav.js"></script>
</head>
<body>
<header>
<nav class="nav wrapper">
  <a href="../index.html">Home</a>
  <a href="introduction.html">Introduction</a>
  <a href="challenges.html">Challenges</a>
  <a href="classical.html">Classical Methods</a>
  <a href="deep-learning.html">Deep Learning</a>
  <a href="multimodal.html">VLM / Multimodal</a>
  <a href="datasets.html">Datasets & Metrics</a>
  <a href="analysis.html">Comparative Analysis</a>
  <a href="future.html">Future</a>
  <a href="bibliography.html">Annotated Bibliography</a>
</nav>
</header>
<main class="wrapper">
  <h1>Datasets & Evaluation Metrics</h1>
  <p class="audio-note">Voice later: <code>assets/audio/datasets.mp3</code></p>
  <audio controls src="../assets/audio/datasets.mp3"></audio>
  <hr/>

  <p>
    Crosswalk research often combines general-purpose driving datasets with smaller, task-specific sets. Large public datasets help with pretraining and benchmarking (e.g., city scenes, semantic labels), while many papers also curate their own crosswalk crops or masks for fine-tuning and evaluation [1][7].
  </p>

  <h2>Commonly Used Datasets</h2>
  <table class="table">
    <tr>
      <th>Dataset</th>
      <th>Type</th>
      <th>Why it’s useful</th>
    </tr>
    <tr>
      <td><strong>Cityscapes</strong></td>
      <td>Urban street scenes with pixel-level labels</td>
      <td>High-quality segmentation ground truth for roads, sidewalks, people, etc.; useful for training/validating segmentation backbones.</td>
    </tr>
    <tr>
      <td><strong>KITTI</strong></td>
      <td>Driving scenes (detection, stereo, tracking)</td>
      <td>Standard benchmarks for object detection/tracking; good for detector backbones and evaluation protocols.</td>
    </tr>
    <tr>
      <td><strong>Task-Specific Crosswalk Sets</strong></td>
      <td>Custom crops / masks from dashcams or maps</td>
      <td>Many papers build small labeled sets of crosswalk patches or masks to fine-tune detectors/segmenters on the actual target task [1][7].</td>
    </tr>
  </table>

  <h2 style="margin-top:22px">Evaluation Metrics</h2>
  <ul>
    <li><strong>Detection:</strong> Precision, Recall, and <em>mean Average Precision (mAP)</em> at IoU thresholds (e.g., 0.5, 0.5:0.95).</li>
    <li><strong>Segmentation:</strong> <em>Intersection-over-Union (IoU)</em> / <em>mean IoU (mIoU)</em>, and pixel accuracy for crosswalk regions.</li>
    <li><strong>Operational:</strong> Frames per second (FPS) / <em>latency</em>, memory/power—important for embedded deployment (e.g., Jetson) [7].</li>
    <li><strong>Robustness:</strong> Performance breakdowns by time of day (day/night), weather, occlusion level, and city/domain to show generalization.</li>
  </ul>

  <div class="card">
    <h3 style="margin-top:0">Reporting Tips (for Reproducibility)</h3>
    <ul>
      <li>State the train/val/test split and whether you fine-tuned on any custom crosswalk set.</li>
      <li>Report both <em>accuracy metrics</em> (mAP/mIoU) and <em>runtime</em> (FPS) so readers see trade-offs.</li>
      <li>Include failure cases (e.g., night scenes, worn paint) to highlight limitations [1].</li>
    </ul>
  </div>

  <figure style="margin-top:18px">
    <img src="../assets/images/crosswalk_placeholder1.png" alt="Dataset illustration" style="width:100%; border-radius:12px; border:1px solid #1f2a44">
    <figcaption>Example: using a generic driving dataset for pretraining, then fine-tuning on crosswalk patches (replace with your own figure + citation).</figcaption>
  </figure>
</main>
<footer>
  <div class="wrapper">Last updated September 28, 2025 • Built for CS661 Project 1 • Author: Anas Niaz</div>
</footer>
</body>
</html>
