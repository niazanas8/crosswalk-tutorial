<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Future Directions • Crosswalk Tutorial</title>
  <link rel="stylesheet" href="../assets/css/styles.css">
  <script defer src="../assets/js/nav.js"></script>
</head>
<body>
<header>
<nav class="nav wrapper">
  <a href="../index.html">Home</a>
  <a href="introduction.html">Introduction</a>
  <a href="challenges.html">Challenges</a>
  <a href="classical.html">Classical Methods</a>
  <a href="deep-learning.html">Deep Learning</a>
  <a href="multimodal.html">VLM / Multimodal</a>
  <a href="datasets.html">Datasets & Metrics</a>
  <a href="analysis.html">Comparative Analysis</a>
  <a href="future.html">Future</a>
  <a href="bibliography.html">Annotated Bibliography</a>
</nav>
</header>

<main class="wrapper">
  <h1>Future Directions & Applications</h1>
  <p class="audio-note">Voice later: <code>assets/audio/future.mp3</code></p>
  <audio controls src="../assets/audio/future.mp3"></audio>
  <hr/>

  <p>
    Research is moving from just detecting painted stripes to <em>understanding the whole crossing situation</em>. Newer systems combine fast detectors with reasoning components that consider traffic lights, vehicles, and pedestrians. Vision-language models (VLMs) are promising here because they can add context and explanations, though they must still meet real-time and reliability needs [3][5].
  </p>

  <div class="grid">
    <div class="card">
      <h3 style="margin-top:0">Multimodal Reasoning</h3>
      <p>Fuse camera input with language prompts, maps, and signals (e.g., light state) to judge if it is <em>safe to cross</em>. Combine CNN/segmentation for geometry with a VLM for context and explanations [3][5].</p>
    </div>
    <div class="card">
      <h3 style="margin-top:0">Night & Adverse Weather</h3>
      <p>Improve robustness using domain adaptation, synthetic nighttime data, and low-light enhancement. Report results broken down by time of day and weather [7].</p>
    </div>
    <div class="card">
      <h3 style="margin-top:0">Efficiency & Deployment</h3>
      <p>Distill or quantize models for embedded devices (e.g., Jetson). Evaluate <strong>accuracy + FPS/latency</strong> together for practical trade-offs [7].</p>
    </div>
    <div class="card">
      <h3 style="margin-top:0">Human-Centered Design</h3>
      <p>Design feedback that helps people: clear audio/tactile cues for blind or low-vision users; concise on-screen explanations for drivers or operators [3].</p>
    </div>
    <div class="card">
      <h3 style="margin-top:0">Data & Benchmarking</h3>
      <p>Create open crosswalk benchmarks with diverse cities and conditions. Include annotations for context (signals, occupancy) to support reasoning research [5].</p>
    </div>
  </div>

  <h2 style="margin-top:22px">Applications</h2>
  <ul>
    <li><strong>Assistive navigation:</strong> smartphone or wearable apps guiding pedestrians with audio prompts.</li>
    <li><strong>Driver assistance / AVs:</strong> crosswalk awareness and safety checks integrated with perception stacks.</li>
    <li><strong>Smart cities:</strong> infrastructure cameras that monitor crossings and alert when conditions are unsafe.</li>
  </ul>

  <figure style="margin-top:18px">
    <img src="../assets/images/crosswalk_placeholder2.png" alt="From detection to scene understanding" style="width:100%; border-radius:12px; border:1px solid #1f2a44">
    <figcaption>Future systems: combine fast geometric detectors with multimodal reasoning and clear user feedback (replace with your own figure + citation).</figcaption>
  </figure>
</main>

<footer>
  <div class="wrapper">Last updated September 28, 2025 • Built for CS661 Project 1 • Author: Anas Niaz</div>
</footer>
</body>
</html>
