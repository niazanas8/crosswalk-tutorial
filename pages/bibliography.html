<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Annotated Bibliography • Crosswalk Tutorial</title>
  <link rel="stylesheet" href="../assets/css/styles.css">
  <script defer src="../assets/js/nav.js"></script>
</head>
<body>
<header>
<nav class="nav wrapper">
  <a href="../index.html">Home</a>
  <a href="introduction.html">Introduction</a>
  <a href="challenges.html">Challenges</a>
  <a href="classical.html">Classical Methods</a>
  <a href="deep-learning.html">Deep Learning</a>
  <a href="multimodal.html">VLM / Multimodal</a>
  <a href="datasets.html">Datasets & Metrics</a>
  <a href="analysis.html">Comparative Analysis</a>
  <a href="future.html">Future</a>
  <a href="bibliography.html" class="active">Annotated Bibliography</a>
</nav>
</header>

<main class="wrapper">
  <h1>Annotated Bibliography</h1>
  <p class="audio-note">Voice-over: <code>../assets/audio/bibliography.mp3</code></p>
  <audio controls src="../assets/audio/bibliography.mp3"></audio>
  <hr/>

  <p>References are listed in ACM citation style, in order of first appearance. Each includes a synopsis and reliability rating.</p>

  <ol>
    <li id="ref-1" class="ref">
      <strong>K. Romić, I. Galić, H. Leventić, and M. Habijan.</strong> 2021. 
      Pedestrian Crosswalk Detection Using a Column and Row Structure Analysis in Assistance Systems for the Visually Impaired. 
      <em>Acta Polytechnica Hungarica</em> 18, 7.  
      DOI: <a href="https://acta.uni-obuda.hu/Romic_Galic_Leventic_Habijan_114.pdf" target="_blank">PDF</a>  
      <br><em>Synopsis:</em> Rule-based detection using row/column intensity and periodicity; evaluates in assistive settings.  
      <br><em>Reliability:</em> Peer-reviewed journal (High).
    </li>

    <li id="ref-2" class="ref">
      <strong>M. Haider et al.</strong> 2025. 
      Advanced Zebra Crosswalk Detection Using Deep Learning Techniques. 
      <em>International Journal of Engineering and Science Education</em> 13, 3.  
      DOI: <a href="https://www.ijese.org/wp-content/uploads/Papers/v13i3/B104214020125.pdf" target="_blank">PDF</a>  
      <br><em>Synopsis:</em> Deep learning pipeline (detection + segmentation) tested under occlusion and varied lighting.  
      <br><em>Reliability:</em> Academic venue (Medium; confirm peer-review).
    </li>

    <li id="ref-3" class="ref">
      <strong>H. Hwang, S. Kwon, Y. Kim, and D. Kim.</strong> 2024. 
      Is it safe to cross? Interpretable Risk Assessment with GPT-4V for Safety-Aware Street Crossing. 
      <em>arXiv preprint</em> arXiv:2402.06794.  
      DOI: <a href="https://arxiv.org/abs/2402.06794" target="_blank">arXiv</a>  
      <br><em>Synopsis:</em> Applies GPT-4V for assessing crossing safety and generating explanations.  
      <br><em>Reliability:</em> Preprint (Medium).
    </li>

    <li id="ref-4" class="ref">
      <strong>M. Liu, J. Jiang, C. Zhu, and X.-C. Yin.</strong> 2023. 
      VLPD: Context-Aware Pedestrian Detection via Vision-Language Semantic Self-Supervision. 
      In <em>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR 2023)</em>.  
      DOI: <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_VLPD_Context-Aware_Pedestrian_Detection_via_Vision-Language_Semantic_Self-Supervision_CVPR_2023_paper.pdf" target="_blank">PDF</a>  
      <br><em>Synopsis:</em> Vision-language supervision improves pedestrian detection under occlusion and crowding.  
      <br><em>Reliability:</em> Top-tier conference (High).
    </li>

    <li id="ref-5" class="ref">
      <strong>C. Zhang et al.</strong> 2023/2024. 
      Vision-Language Models in Autonomous Driving: A Survey and Outlook. 
      <em>arXiv preprint</em> arXiv:2310.14414.  
      DOI: <a href="https://arxiv.org/abs/2310.14414" target="_blank">arXiv</a>  
      <br><em>Synopsis:</em> Comprehensive survey of VLMs in autonomous driving, covering perception, planning, and open challenges.  
      <br><em>Reliability:</em> Survey preprint (Medium).
    </li>

    <li id="ref-6" class="ref">
      <strong>J. Fan, J. Wu, J. Gao, J. Yu, Y. Wang, H. Chu, and B. Gao.</strong> 2024. 
      MLLM-SUL: Multimodal LLM for Scene Understanding and Risk Localization in Traffic. 
      <em>arXiv preprint</em> arXiv:2412.19406.  
      DOI: <a href="https://arxiv.org/abs/2412.19406" target="_blank">arXiv</a>  
      <br><em>Synopsis:</em> Demonstrates a multimodal LLM that narrates traffic scenes and highlights risk zones.  
      <br><em>Reliability:</em> Preprint (Medium).
    </li>

    <li id="ref-7" class="ref">
      <strong>Z.-D. Zhang, M.-L. Tan, Z.-C. Lan, H.-C. Liu, L. Pei, and W.-X. Yu.</strong> 2022. 
      CDNet: A Real-Time and Robust Crosswalk Detection Network on Jetson Nano Based on YOLOv5. 
      <em>Neural Computing and Applications</em>.  
      DOI: <a href="https://doi.org/10.1007/s00521-022-07007-9" target="_blank">10.1007/s00521-022-07007-9</a>  
      <br><em>Synopsis:</em> YOLOv5-based crosswalk detector optimized for Jetson; reports strong FPS and accuracy.  
      <br><em>Reliability:</em> Peer-reviewed journal (High).
    </li>
  </ol>

  <div class="notice" style="margin-top:16px">
    Tip: use inline citations like 
    <code>&lt;a class="cite" href="bibliography.html#ref-3"&gt;[3]&lt;/a&gt;</code> 
    in your tutorial pages so readers can jump here.
  </div>
</main>

<footer>
  <div class="wrapper">Last updated September 28, 2025 • Built for CS661 Project 1 • Author: Anas Niaz</div>
</footer>

<script defer src="../assets/js/cite.js"></script>
</body>
</html>
