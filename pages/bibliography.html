<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Annotated Bibliography • Crosswalk Tutorial</title>
  <link rel="stylesheet" href="../assets/css/styles.css">
  <script defer src="../assets/js/nav.js"></script>
</head>
<body>
<header>
<nav class="nav wrapper">
  <a href="../index.html">Home</a>
  <a href="introduction.html">Introduction</a>
  <a href="challenges.html">Challenges</a>
  <a href="classical.html">Classical Methods</a>
  <a href="deep-learning.html">Deep Learning</a>
  <a href="multimodal.html">VLM / Multimodal</a>
  <a href="datasets.html">Datasets & Metrics</a>
  <a href="analysis.html">Comparative Analysis</a>
  <a href="future.html">Future</a>
  <a href="bibliography.html" class="active">Annotated Bibliography</a>
</nav>
</header>

<main class="wrapper">
  <h1>Annotated Bibliography</h1>
  <p class="audio-note">Voice-over: <code>../assets/audio/bibliography.mp3</code></p>
  <audio controls src="../assets/audio/bibliography.mp3"></audio>
  <hr/>

  <p>References are listed in order of first appearance. Each entry includes a brief synopsis and a reliability note.</p>

  <ol>
    <li id="ref-1" class="ref">
      <strong>Romić, K.; Galić, I.; Leventić, H.; Habijan, M.</strong> (2021).
      <em>Pedestrian Crosswalk Detection Using a Column and Row Structure Analysis in Assistance Systems for the Visually Impaired.</em>
      <span>Acta Polytechnica Hungarica 18(7).</span>
      <br>
      <em>Link:</em> <a href="https://acta.uni-obuda.hu/Romic_Galic_Leventic_Habijan_114.pdf" target="_blank" rel="noopener">PDF (Acta)</a>
      <a class="backref" href="javascript:history.back()">↩︎ back</a>
      <br>
      <em>Synopsis:</em> Rule-based detection using row/column intensity and periodicity; evaluates in assistive settings.
      <br>
      <em>Reliability:</em> Peer-reviewed journal (High).
    </li>

    <li id="ref-2" class="ref">
      <strong>Haider, M.; et al.</strong> (2025).
      <em>Advanced Zebra Crosswalk Detection Using Deep Learning Techniques.</em>
      <span>International Journal of Engineering and Science Education 13(3).</span>
      <br>
      <em>Link:</em> <a href="https://www.ijese.org/wp-content/uploads/Papers/v13i3/B104214020125.pdf" target="_blank" rel="noopener">PDF (IJese)</a>
      <a class="backref" href="javascript:history.back()">↩︎ back</a>
      <br>
      <em>Synopsis:</em> DL pipeline (detection + segmentation) under occlusion / varied lighting.
      <br>
      <em>Reliability:</em> Academic venue (Medium; verify peer-review status).
    </li>

    <li id="ref-3" class="ref">
      <strong>Hwang, H.; Kwon, S.; Kim, Y.; Kim, D.</strong> (2024).
      <em>Is it safe to cross? Interpretable Risk Assessment with GPT-4V for Safety-Aware Street Crossing.</em>
      <span>arXiv:2402.06794.</span>
      <br>
      <em>Link:</em> <a href="https://arxiv.org/abs/2402.06794" target="_blank" rel="noopener">arXiv (abstract)</a> ·
      <a href="https://arxiv.org/pdf/2402.06794" target="_blank" rel="noopener">PDF</a>
      <a class="backref" href="javascript:history.back()">↩︎ back</a>
      <br>
      <em>Synopsis:</em> Uses GPT-4V to assess crossing safety and produce explanations.
      <br>
      <em>Reliability:</em> Preprint (Medium).
    </li>

    <li id="ref-4" class="ref">
      <strong>Liu, M.; Jiang, J.; Zhu, C.; Yin, X.-C.</strong> (2023).
      <em>VLPD: Context-Aware Pedestrian Detection via Vision-Language Semantic Self-Supervision.</em>
      <span>CVPR 2023.</span>
      <br>
      <em>Link:</em>
      <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_VLPD_Context-Aware_Pedestrian_Detection_via_Vision-Language_Semantic_Self-Supervision_CVPR_2023_paper.pdf" target="_blank" rel="noopener">PDF (CVPR Open Access)</a> ·
      <a href="https://openaccess.thecvf.com/content/CVPR2023/html/Liu_VLPD_Context-Aware_Pedestrian_Detection_via_Vision-Language_Semantic_Self-Supervision_CVPR_2023_paper.html" target="_blank" rel="noopener">Project Page</a>
      <a class="backref" href="javascript:history.back()">↩︎ back</a>
      <br>
      <em>Synopsis:</em> Vision-language supervision improves pedestrian detection in occluded/crowded scenes.
      <br>
      <em>Reliability:</em> Top-tier conference (High).
    </li>

    <li id="ref-5" class="ref">
      <strong>Zhang, C.; et al.</strong> (2023/2024).
      <em>Vision-Language Models in Autonomous Driving: A Survey and Outlook.</em>
      <span>arXiv:2310.14414.</span>
      <br>
      <em>Link:</em> <a href="https://arxiv.org/abs/2310.14414" target="_blank" rel="noopener">arXiv (abstract)</a> ·
      <a href="https://arxiv.org/pdf/2310.14414" target="_blank" rel="noopener">PDF</a>
      <a class="backref" href="javascript:history.back()">↩︎ back</a>
      <br>
      <em>Synopsis:</em> Survey of VLMs for driving (perception, planning, decision-making) + open challenges.
      <br>
      <em>Reliability:</em> Survey preprint (Medium).
    </li>

    <li id="ref-6" class="ref">
      <strong>Fan, J.; Wu, J.; Gao, J.; Yu, J.; Wang, Y.; Chu, H.; Gao, B.</strong> (2024).
      <em>MLLM-SUL: Multimodal LLM for Scene Understanding and Risk Localization in Traffic.</em>
      <span>arXiv:2412.19406.</span>
      <br>
      <em>Link:</em> <a href="https://arxiv.org/abs/2412.19406" target="_blank" rel="noopener">arXiv (abstract)</a> ·
      <a href="https://arxiv.org/pdf/2412.19406" target="_blank" rel="noopener">PDF</a> ·
      <a href="https://github.com/fjq-tongji/MLLM-SUL" target="_blank" rel="noopener">GitHub</a>
      <a class="backref" href="javascript:history.back()">↩︎ back</a>
      <br>
      <em>Synopsis:</em> Multimodal LLM narrates traffic scenes and highlights potential risk zones.
      <br>
      <em>Reliability:</em> Preprint (Medium).
    </li>

    <li id="ref-7" class="ref">
      <strong>Zhang, Z.-D.; Tan, M.-L.; Lan, Z.-C.; Liu, H.-C.; Pei, L.; Yu, W.-X.</strong> (2022).
      <em>CDNet: a real-time and robust crosswalk detection network on Jetson Nano based on YOLOv5.</em>
      <span>Neural Computing & Applications.</span>
      <br>
      <em>Link:</em>
      <a href="https://link.springer.com/article/10.1007/s00521-022-07007-9" target="_blank" rel="noopener">Springer (article)</a> ·
      <a href="https://doi.org/10.1007/s00521-022-07007-9" target="_blank" rel="noopener">DOI</a> ·
      <a href="https://github.com/zhangzhengde0225/CDNet" target="_blank" rel="noopener">GitHub (code)</a>
      <a class="backref" href="javascript:history.back()">↩︎ back</a>
      <br>
      <em>Synopsis:</em> YOLOv5-based crosswalk detector optimized for Jetson; reports strong FPS and accuracy.
      <br>
      <em>Reliability:</em> Peer-reviewed journal (High).
    </li>
  </ol>

  <div class="notice" style="margin-top:16px">
    Tip: use inline citations like <code>&lt;a class="cite" href="bibliography.html#ref-3" data-cite="Hwang et al., 2024"&gt;[3]&lt;/a&gt;</code> so readers can jump here.
  </div>
</main>

<footer>
  <div class="wrapper">Last updated September 28, 2025 • Built for CS661 Project 1 • Author: Anas Niaz</div>
</footer>

<!-- Optional: highlight a ref when navigated to -->
<script defer src="../assets/js/cite.js"></script>
</body>
</html>
