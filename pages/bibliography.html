<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Annotated Bibliography • Crosswalk Tutorial</title>
  <link rel="stylesheet" href="../assets/css/styles.css">
  <script defer src="../assets/js/nav.js"></script>
</head>
<body>
<header>
  <nav class="nav wrapper">
    <a href="../index.html">Home</a>
    <a href="introduction.html">Introduction</a>
    <a href="challenges.html">Challenges</a>
    <a href="classical.html">Classical Methods</a>
    <a href="deep-learning.html">Deep Learning</a>
    <a href="multimodal.html">VLM / Multimodal</a>
    <a href="datasets.html">Datasets & Metrics</a>
    <a href="analysis.html">Comparative Analysis</a>
    <a href="future.html">Future</a>
    <a href="bibliography.html">Annotated Bibliography</a>
  </nav>
</header>

<main class="wrapper">
  <h1>Annotated Bibliography</h1>
  <p class="audio-note">Voice later: <code>assets/audio/bibliography.mp3</code></p>
  <audio controls src="../assets/audio/bibliography.mp3"></audio>
  <hr/>

  <p>References are listed in order of first appearance. Each entry includes a brief synopsis and a reliability note (journal/conference vs. preprint). Update links and formatting (ACM/APA) if your instructor requests a specific style.</p>

  <ol>
    <li id="ref-1">
      <strong>Romić, K.; Galić, I.; Leventić, H.; Habijan, M.</strong> (2021).
      <em>Pedestrian Crosswalk Detection Using a Column and Row Structure Analysis in Assistance Systems for the Visually Impaired.</em>
      <span>Acta Polytechnica Hungarica 18(7).</span>
      <br>
      <em>Link:</em> <a href="https://acta.uni-obuda.hu/Romic_Galic_Leventic_Habijan_114.pdf" target="_blank" rel="noopener">PDF</a>
      <br>
      <em>Synopsis:</em> Proposes a rule-based approach using row/column intensity patterns and periodicity to detect zebra-style crosswalks. Includes evaluation in assistive settings.
      <br>
      <em>Reliability:</em> Peer-reviewed journal (High).
    </li>

    <li id="ref-2">
      <strong>Haider, M.; et al.</strong> (2025).
      <em>Advanced Zebra Crosswalk Detection Using Deep Learning Techniques.</em>
      <span>International Journal of Engineering and Science Education 13(3).</span>
      <br>
      <em>Link:</em> <a href="https://www.ijese.org/wp-content/uploads/Papers/v13i3/B104214020125.pdf" target="_blank" rel="noopener">PDF</a>
      <br>
      <em>Synopsis:</em> Presents a deep-learning pipeline (detection/segmentation) for zebra crosswalks with experiments under occlusion and varied lighting.
      <br>
      <em>Reliability:</em> Academic venue (Medium; verify peer-review status).
    </li>

    <li id="ref-3">
      <strong>Hwang, H.; Kwon, S.; Kim, Y.; Kim, D.</strong> (2024).
      <em>Is it safe to cross? Interpretable Risk Assessment with GPT-4V for Safety-Aware Street Crossing.</em>
      <span>arXiv:2402.06794.</span>
      <br>
      <em>Link:</em> <a href="https://arxiv.org/abs/2402.06794" target="_blank" rel="noopener">arXiv</a>
      <br>
      <em>Synopsis:</em> Uses a large multimodal model (GPT-4V) to rate street-crossing safety and generate short natural-language explanations, showing how VLMs add contextual reasoning.
      <br>
      <em>Reliability:</em> Preprint (Medium).
    </li>

    <li id="ref-4">
      <strong>Liu, M.; Jiang, J.; Zhu, C.; Yin, X.-C.</strong> (2023).
      <em>VLPD: Context-Aware Pedestrian Detection via Vision-Language Semantic Self-Supervision.</em>
      <span>CVPR 2023.</span>
      <br>
      <em>Link:</em> (Add official CVPR/IEEE link or arXiv if available)
      <br>
      <em>Synopsis:</em> Leverages vision-language signals for self-supervised context, improving pedestrian detection under occlusion/crowds—relevant to road scene understanding.
      <br>
      <em>Reliability:</em> Top-tier conference (High).
    </li>

    <li id="ref-5">
      <strong>Zhang, C.; et al.</strong> (2023/2024).
      <em>Vision-Language Models in Autonomous Driving: A Survey and Outlook.</em>
      <span>arXiv:2310.14414.</span>
      <br>
      <em>Link:</em> <a href="https://arxiv.org/abs/2310.14414" target="_blank" rel="noopener">arXiv</a>
      <br>
      <em>Synopsis:</em> Survey of VLM applications to driving: perception, planning, decision-making, and data generation; summarizes challenges like latency and domain shift.
      <br>
      <em>Reliability:</em> Survey preprint (Medium).
    </li>

    <li id="ref-6">
      <strong>Fan, J.; Wu, J.; Gao, J.; Yu, J.; Wang, Y.; Chu, H.; Gao, B.</strong> (2024).
      <em>MLLM-SUL: Multimodal LLM for Scene Understanding and Risk Localization in Traffic.</em>
      <span>arXiv (preprint).</span>
      <br>
      <em>Link:</em> (Add arXiv/DOI link)
      <br>
      <em>Synopsis:</em> Describes a multimodal LLM that narrates the traffic scene and highlights potential risks from front-view images—aligned with “is it safe to cross?” reasoning.
      <br>
      <em>Reliability:</em> Preprint (Medium).
    </li>

    <li id="ref-7">
      <strong>Yang, Z.; et al.</strong> (2022).
      <em>CDNet: Real-time and Robust Crosswalk Detection on Jetson with YOLOv5.</em>
      <span>Neural Computing & Applications (Springer).</span>
      <br>
      <em>Link:</em> (Add Springer/DOI link)
      <br>
      <em>Synopsis:</em> Focuses on practical deployment: YOLOv5-based detection optimized for embedded devices; reports accuracy and FPS/latency trade-offs.
      <br>
      <em>Reliability:</em> Peer-reviewed journal (High).
    </li>
  </ol>

  <div class="notice" style="margin-top:16px">
    Tip: keep the numbering stable across pages. When you cite in text (e.g., <code>[3]</code>), link to these anchors like <code>&lt;a href="bibliography.html#ref-3"&gt;[3]&lt;/a&gt;</code>.
  </div>
</main>

<footer>
  <div class="wrapper">Last updated September 28, 2025 • Built for CS661 Project 1 • Author: Anas Niaz</div>
</footer>
</body>
</html>
